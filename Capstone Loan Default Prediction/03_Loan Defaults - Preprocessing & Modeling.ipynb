{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd592f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from numpy import linspace\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler \n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \\\n",
    "    confusion_matrix, mean_squared_error, r2_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "from sklearn.svm import SVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ea8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/03_eda_loans50k.csv', index_col=[0])\n",
    "top_features = pd.read_csv('data/top_features.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55249da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>term</th>\n",
       "      <th>length</th>\n",
       "      <th>home</th>\n",
       "      <th>income</th>\n",
       "      <th>verified</th>\n",
       "      <th>status</th>\n",
       "      <th>reason</th>\n",
       "      <th>state</th>\n",
       "      <th>debtIncRat</th>\n",
       "      <th>...</th>\n",
       "      <th>totalAcc</th>\n",
       "      <th>totalBal</th>\n",
       "      <th>totalRevLim</th>\n",
       "      <th>accOpen24</th>\n",
       "      <th>avgBal</th>\n",
       "      <th>bcOpen</th>\n",
       "      <th>bcRatio</th>\n",
       "      <th>totalRevBal</th>\n",
       "      <th>totalBcLim</th>\n",
       "      <th>totalIlLim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loanID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188861</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>3 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>49966.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Default</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Quartile_3</td>\n",
       "      <td>30.05</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>48054.0</td>\n",
       "      <td>8100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4369.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>48054.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>52529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517703</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Current</td>\n",
       "      <td>Debt Consolidation</td>\n",
       "      <td>Quartile_3</td>\n",
       "      <td>8.74</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>265838.0</td>\n",
       "      <td>33400.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22153.0</td>\n",
       "      <td>19756.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>11844.0</td>\n",
       "      <td>31600.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268587</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Current</td>\n",
       "      <td>Debt Consolidation</td>\n",
       "      <td>Quartile_3</td>\n",
       "      <td>14.50</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>850150.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121450.0</td>\n",
       "      <td>7686.0</td>\n",
       "      <td>83.2</td>\n",
       "      <td>173260.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579902</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>2 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Current</td>\n",
       "      <td>Debt Consolidation</td>\n",
       "      <td>Quartile_3</td>\n",
       "      <td>14.14</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>20671.0</td>\n",
       "      <td>14200.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1723.0</td>\n",
       "      <td>11061.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>20671.0</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>32764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617630</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Current</td>\n",
       "      <td>Debt Consolidation</td>\n",
       "      <td>Quartile_3</td>\n",
       "      <td>5.14</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>128034.0</td>\n",
       "      <td>14600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18291.0</td>\n",
       "      <td>5904.0</td>\n",
       "      <td>58.4</td>\n",
       "      <td>8475.0</td>\n",
       "      <td>14200.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         amount        term     length      home    income      verified  \\\n",
       "loanID                                                                     \n",
       "188861   8000.0   36 months    3 years      RENT   49966.0      Verified   \n",
       "517703  11000.0   36 months  10+ years  MORTGAGE   50000.0  Not Verified   \n",
       "268587  35000.0   36 months  10+ years      RENT  360000.0      Verified   \n",
       "579902  20000.0   60 months    2 years  MORTGAGE   60000.0      Verified   \n",
       "617630  12000.0   60 months  10+ years  MORTGAGE   64000.0      Verified   \n",
       "\n",
       "         status              reason       state  debtIncRat  ...  totalAcc  \\\n",
       "loanID                                                       ...             \n",
       "188861  Default         Credit Card  Quartile_3       30.05  ...      15.0   \n",
       "517703  Current  Debt Consolidation  Quartile_3        8.74  ...      15.0   \n",
       "268587  Current  Debt Consolidation  Quartile_3       14.50  ...      14.0   \n",
       "579902  Current  Debt Consolidation  Quartile_3       14.14  ...      48.0   \n",
       "617630  Current  Debt Consolidation  Quartile_3        5.14  ...      22.0   \n",
       "\n",
       "        totalBal  totalRevLim  accOpen24    avgBal   bcOpen  bcRatio  \\\n",
       "loanID                                                                 \n",
       "188861   48054.0       8100.0        8.0    4369.0     43.0     95.7   \n",
       "517703  265838.0      33400.0        4.0   22153.0  19756.0     37.5   \n",
       "268587  850150.0      83000.0        2.0  121450.0   7686.0     83.2   \n",
       "579902   20671.0      14200.0        8.0    1723.0  11061.0      1.2   \n",
       "617630  128034.0      14600.0        3.0   18291.0   5904.0     58.4   \n",
       "\n",
       "        totalRevBal  totalBcLim  totalIlLim  \n",
       "loanID                                       \n",
       "188861      48054.0      1000.0     52529.0  \n",
       "517703      11844.0     31600.0         0.0  \n",
       "268587     173260.0     83000.0    100000.0  \n",
       "579902      20671.0     11200.0     32764.0  \n",
       "617630       8475.0     14200.0         0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12f111",
   "metadata": {},
   "source": [
    "**Feature Encoding using One Hot Encoding on all categorical data. This took the number of features from 25 to 49.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda54605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49184 entries, 188861 to 500393\n",
      "Data columns (total 25 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   amount       49184 non-null  float64\n",
      " 1   term         49184 non-null  object \n",
      " 2   length       49184 non-null  object \n",
      " 3   home         49184 non-null  object \n",
      " 4   income       49184 non-null  float64\n",
      " 5   verified     49184 non-null  object \n",
      " 6   status       49184 non-null  object \n",
      " 7   reason       49184 non-null  object \n",
      " 8   state        49184 non-null  object \n",
      " 9   debtIncRat   49184 non-null  float64\n",
      " 10  delinq2yr    49184 non-null  float64\n",
      " 11  inq6mth      49184 non-null  float64\n",
      " 12  openAcc      49184 non-null  float64\n",
      " 13  pubRec       49184 non-null  float64\n",
      " 14  revolRatio   49184 non-null  float64\n",
      " 15  totalAcc     49184 non-null  float64\n",
      " 16  totalBal     49184 non-null  float64\n",
      " 17  totalRevLim  49184 non-null  float64\n",
      " 18  accOpen24    49184 non-null  float64\n",
      " 19  avgBal       49184 non-null  float64\n",
      " 20  bcOpen       49184 non-null  float64\n",
      " 21  bcRatio      49184 non-null  float64\n",
      " 22  totalRevBal  49184 non-null  float64\n",
      " 23  totalBcLim   49184 non-null  float64\n",
      " 24  totalIlLim   49184 non-null  float64\n",
      "dtypes: float64(18), object(7)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a334f2a",
   "metadata": {},
   "source": [
    "**Before encoding, I need to map the length feature to preserve the hierarchical order of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc3921f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 years' '10+ years' '2 years' '< 1 year' '9 years' '5 years' '1 year'\n",
      " '4 years' '8 years' '7 years' '6 years' '0 years']\n"
     ]
    }
   ],
   "source": [
    "# Confirm unique values to be mapped.\n",
    "print(df['length'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2512941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_map = {\"0 years\": 0, \"< 1 year\": 1, \"1 year\": 2, \"2 years\": 3, \"3 years\": 4,\\\n",
    "              \"4 years\": 5, \"5 years\": 6, \"6 years\": 7, \"7 years\": 8,\\\n",
    "              \"8 years\": 9, \"9 years\": 10, \"10+ years\": 11}\n",
    "df['length'] = df['length'].map(length_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437de0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 11,  3,  1, 10,  6,  2,  5,  9,  8,  7,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check new variables\n",
    "df.length.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c578c0bc",
   "metadata": {},
   "source": [
    "**Split data into targt variable (y) and features (X) Then use dummy encoding to transform categorical features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6bb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and then get dummies\n",
    "X = df.drop('status', axis=1)\n",
    "y = df['status']\n",
    "# Encode y\n",
    "y = y.replace({'Current': 0, 'Default': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fc0bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amount', 'length', 'income', 'debtIncRat', 'delinq2yr', 'inq6mth',\n",
       "       'openAcc', 'pubRec', 'revolRatio', 'totalAcc', 'totalBal',\n",
       "       'totalRevLim', 'accOpen24', 'avgBal', 'bcOpen', 'bcRatio',\n",
       "       'totalRevBal', 'totalBcLim', 'totalIlLim', 'term_ 60 months',\n",
       "       'home_OWN', 'home_RENT', 'verified_Verified', 'reason_Credit Card',\n",
       "       'reason_Debt Consolidation', 'reason_Home Improvement', 'reason_House',\n",
       "       'reason_Major Purchase', 'reason_Medical', 'reason_Moving',\n",
       "       'reason_Other', 'reason_Renewable Energy', 'reason_Small Business',\n",
       "       'reason_Vacation', 'reason_Wedding', 'state_Quartile_2',\n",
       "       'state_Quartile_3', 'state_Quartile_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummies and check new feautures\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3998ad3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37381b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uint8      19\n",
       "float64    18\n",
       "int64       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20f954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change uint8 data tpes to int\n",
    "uint8_cols = X.select_dtypes(include='uint8').columns\n",
    "X[uint8_cols] = X[uint8_cols].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898539a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int64      20\n",
       "float64    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm uint8 have been changes to int64\n",
    "X.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f3cfa",
   "metadata": {},
   "source": [
    "**Train / Test Split & Scale using StandardScaler.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ec931da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# initialize a StandardScaler object and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# apply the scaling transformation to both the training and test data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3165f3",
   "metadata": {},
   "source": [
    "**I wanted to check to see if there were a similar amount of defaults in the training and test set. The ratio is close enough.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41b3883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of defaults in the training set is 15.52% and the ratio in the test set is 15.15%\n"
     ]
    }
   ],
   "source": [
    "# Find ratios of defaults in the training set\n",
    "ratio_defaults_train = y_train.sum() / len(y_train)\n",
    "# Find ratios of defaults in the test set\n",
    "ratio_defaults_test = y_test.sum() / len(y_test)\n",
    "print(f\"The ratio of defaults in the training set is {ratio_defaults_train:.2%} and the ratio in the test set is \\\n",
    "{ratio_defaults_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fb222",
   "metadata": {},
   "source": [
    "**Data is ready for modeling. I'm going to try 4 algorithms and pick the top one or two to move forward with: <br> Logistic Regression <br> Random Forest Classifier <br> Gradient Boosting Classifier <br> Support Vector Machines (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f058691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6397397668744917\n",
      "Precision: 0.2421299397186872\n",
      "Recall: 0.646690518783542\n",
      "F1 score: 0.3523391812865497\n",
      "ROC-AUC score: 0.6425944606697263\n",
      "Total Time: 0.13979196548461914\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create a logistic regression object\n",
    "lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train the model on the training data (scaled data)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data (scaled data)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the metrics\n",
    "lr_acc = accuracy_score(y_test, y_pred)\n",
    "lr_prec = precision_score(y_test, y_pred)\n",
    "lr_rec = recall_score(y_test, y_pred)\n",
    "lr_f1 = f1_score(y_test, y_pred)\n",
    "lr_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "end_time = time.time()\n",
    "lr_total_time = end_time - start_time\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", lr_acc)\n",
    "print(\"Precision:\", lr_prec)\n",
    "print(\"Recall:\", lr_rec)\n",
    "print(\"F1 score:\", lr_f1)\n",
    "print(\"ROC-AUC score:\", lr_roc_auc)\n",
    "print(\"Total Time:\", lr_total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edca7373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8484684196259149\n",
      "Precision: 0.5\n",
      "Recall: 0.0026833631484794273\n",
      "F1 score: 0.005338078291814947\n",
      "ROC-AUC score: 0.5011020649608212\n",
      "Total Time: 8.041346073150635\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create a random forest classifier object\n",
    "rfc = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train the model on the training data (unscaled)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data (unscaled)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "rfc_acc = accuracy_score(y_test, y_pred)\n",
    "rfc_prec = precision_score(y_test, y_pred)\n",
    "rfc_rec = recall_score(y_test, y_pred)\n",
    "rfc_f1 = f1_score(y_test, y_pred)\n",
    "rfc_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "end_time = time.time()\n",
    "rfc_total_time = end_time - start_time\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", rfc_acc)\n",
    "print(\"Precision:\", rfc_prec)\n",
    "print(\"Recall:\", rfc_rec)\n",
    "print(\"F1 score:\", rfc_f1)\n",
    "print(\"ROC-AUC score:\", rfc_roc_auc)\n",
    "print('Total Time:', rfc_total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0cfeb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8484684196259149\n",
      "Precision: 0.5\n",
      "Recall: 0.007155635062611807\n",
      "F1 score: 0.014109347442680775\n",
      "ROC-AUC score: 0.5029388398955232\n",
      "Total Time: 11.673396110534668\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create a gradient boosting classifier object\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data (unscaled)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data (unscaled)\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "gb_acc = accuracy_score(y_test, y_pred)\n",
    "gb_prec = precision_score(y_test, y_pred)\n",
    "gb_rec = recall_score(y_test, y_pred)\n",
    "gb_f1 = f1_score(y_test, y_pred)\n",
    "gb_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "end_time = time.time()\n",
    "gb_total_time = end_time - start_time\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", gb_acc)\n",
    "print(\"Precision:\", gb_prec)\n",
    "print(\"Recall:\", gb_rec)\n",
    "print(\"F1 score:\", gb_f1)\n",
    "print(\"ROC-AUC score:\", gb_roc_auc)\n",
    "print('Total Time:', gb_total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e53c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Initialize the SVC with default hyperparameters\n",
    "svc = SVC(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train the SVC on the training set (scaled)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set (scaled)\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the metrics\n",
    "svc_acc = accuracy_score(y_test, y_pred)\n",
    "svc_prec = precision_score(y_test, y_pred)\n",
    "svc_rec = recall_score(y_test, y_pred)\n",
    "svc_f1 = f1_score(y_test, y_pred)\n",
    "svc_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "end_time = time.time()\n",
    "svc_total_time = end_time - start_time\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", svc_acc)\n",
    "print(\"Precision:\", svc_prec)\n",
    "print(\"Recall:\", svc_rec)\n",
    "print(\"F1 score:\", svc_f1)\n",
    "print(\"ROC-AUC score:\", svc_roc_auc)\n",
    "print('Total Time:', svc_total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0eba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of scores for each model\n",
    "scores_dict = {\n",
    "    'Model': ['LR','RFC', 'GB', 'SVC'],\n",
    "    'Accuracy': [lr_acc, rfc_acc, gb_acc, svc_acc],\n",
    "    'Precision': [lr_prec, rfc_prec, gb_prec, svc_prec],\n",
    "    'Recall': [lr_rec, rfc_rec, gb_rec, svc_rec],\n",
    "    'F1 Score': [lr_f1, rfc_f1, gb_f1, svc_f1],\n",
    "    'ROC-AUC Score': [lr_roc_auc, rfc_roc_auc, gb_roc_auc, svc_roc_auc,],\n",
    "    'Total Time': [lr_total_time, rfc_total_time, gb_total_time, svc_total_time]\n",
    "}\n",
    "\n",
    "# create the DataFrame\n",
    "scores_df = pd.DataFrame(scores_dict)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d69e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the DataFrame by 'Recall' column\n",
    "scores_df_sorted = scores_df.sort_values(by='Recall', ascending=False)\n",
    "\n",
    "# plot a horizontal bar chart\n",
    "ax = scores_df_sorted.plot(x='Model', y='Recall', kind='bar', legend=False, color='teal')\n",
    "\n",
    "# Add title and labels for x-axis & y-axis\n",
    "ax.set_title('Linear Regression Narrowly Beats Support Vector Machine')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Recall Score')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Add horizontal lines behind the bars.\n",
    "for i in range(0, 7):\n",
    "    plt.axhline(y=i/10, color='lightblue', zorder=-1, linewidth=0.5)\n",
    "    \n",
    "# Add a horizontal line at 0.0\n",
    "plt.axhline(y=0.0, color='lightblue', zorder=-1, linewidth=1.50)\n",
    "\n",
    "# Remove the spines of the plot\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Remove the little ticks on the y-axis\n",
    "plt.tick_params(axis='y', which='both', length=0)\n",
    "\n",
    "# Save the figure as a PNG image with a specified resolution and tight bounding box\n",
    "plt.savefig('Images/Recall Scores by Model', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the DataFrame by 'Total Time' column\n",
    "times_df_sorted = scores_df.sort_values(by='Total Time', ascending=False)\n",
    "\n",
    "# plot a horizontal bar chart\n",
    "ax = times_df_sorted.plot(x='Model', y='Total Time', kind='bar', legend=False, color='teal')\n",
    "\n",
    "# Add title and labels for x-axis & y-axis\n",
    "ax.set_title(\"Linear Regression Is So Fast, \\nYou Can't See It.\")\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Total Time (seconds)')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Add horizontal lines behind the bars.\n",
    "for i in range(0, 201, 25):\n",
    "    plt.axhline(y=i, color='lightblue', zorder=-1, linewidth=0.5)\n",
    "    \n",
    "# Add a horizontal line at 0.0\n",
    "plt.axhline(y=0.0, color='lightblue', zorder=-1, linewidth=1.50)\n",
    "\n",
    "# Remove the spines of the plot\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Remove the little ticks on the y-axis\n",
    "plt.tick_params(axis='y', which='both', length=0)\n",
    "\n",
    "# Save the figure as a PNG image with a specified resolution and tight bounding box\n",
    "plt.savefig('Images/Total Time by Model', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ed43e",
   "metadata": {},
   "source": [
    "**Because we care most about detecting true loan defaults, we will move forward with logistic regression as it has the highest recall score. SVC is a close second but it takes much longer to run the model and is still not performing as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e47b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create oversampled training set using synthetic data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# create oversampled training set\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# create undersampled training set\n",
    "rus = RandomUnderSampler(random_state=42, replacement=True)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92cdddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use synthetic oversampling\n",
    "\n",
    "# Train the model on the training data (scaled data)\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test data (scaled data)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the metrics\n",
    "sm_lr_acc = accuracy_score(y_test, y_pred)\n",
    "sm_lr_prec = precision_score(y_test, y_pred)\n",
    "sm_lr_rec = recall_score(y_test, y_pred)\n",
    "sm_lr_f1 = f1_score(y_test, y_pred)\n",
    "sm_lr_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", sm_lr_acc)\n",
    "print(\"Precision:\", sm_lr_prec)\n",
    "print(\"Recall:\", sm_lr_rec)\n",
    "print(\"F1 score:\", sm_lr_f1)\n",
    "print(\"ROC-AUC score:\", sm_lr_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7608e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random oversampling\n",
    "\n",
    "# Train the model on the training data (scaled data)\n",
    "lr.fit(X_train_ros, y_train_ros)\n",
    "\n",
    "# Make predictions on the test data (scaled data)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the metrics\n",
    "ros_lr_acc = accuracy_score(y_test, y_pred)\n",
    "ros_lr_prec = precision_score(y_test, y_pred)\n",
    "ros_lr_rec = recall_score(y_test, y_pred)\n",
    "ros_lr_f1 = f1_score(y_test, y_pred)\n",
    "ros_lr_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", ros_lr_acc)\n",
    "print(\"Precision:\", ros_lr_prec)\n",
    "print(\"Recall:\", ros_lr_rec)\n",
    "print(\"F1 score:\", ros_lr_f1)\n",
    "print(\"ROC-AUC score:\", ros_lr_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random udersampling\n",
    "\n",
    "# Train the model on the training data (scaled data)\n",
    "lr.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# Make predictions on the test data (scaled data)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the metrics\n",
    "rus_lr_acc = accuracy_score(y_test, y_pred)\n",
    "rus_lr_prec = precision_score(y_test, y_pred)\n",
    "rus_lr_rec = recall_score(y_test, y_pred)\n",
    "rus_lr_f1 = f1_score(y_test, y_pred)\n",
    "rus_lr_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", rus_lr_acc)\n",
    "print(\"Precision:\", rus_lr_prec)\n",
    "print(\"Recall:\", rus_lr_rec)\n",
    "print(\"F1 score:\", rus_lr_f1)\n",
    "print(\"ROC-AUC score:\", rus_lr_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b770ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of scores for each model\n",
    "lr_scores_dict = {\n",
    "    'Model': ['LR','LR: Smote', 'LR: ROS', 'LR: RUS'],\n",
    "    'Accuracy': [lr_acc, sm_lr_acc, ros_lr_acc, rus_lr_acc],\n",
    "    'Precision': [lr_prec, sm_lr_prec, ros_lr_prec, rus_lr_prec],\n",
    "    'Recall': [lr_rec, sm_lr_rec, ros_lr_rec, rus_lr_rec],\n",
    "    'F1 Score': [lr_f1, sm_lr_f1, ros_lr_f1, rus_lr_f1],\n",
    "    'ROC-AUC Score': [lr_roc_auc, sm_lr_roc_auc, ros_lr_roc_auc, rus_lr_roc_auc]\n",
    "}\n",
    "\n",
    "# create the DataFrame\n",
    "lr_scores_df = pd.DataFrame(lr_scores_dict)\n",
    "lr_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e161ca7",
   "metadata": {},
   "source": [
    "**I will use SMOTE synthetic oversampling to tune my hyperparameters. It had the highest scores in all metrics except accuracy which I am less concerned with.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599778d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression object\n",
    "lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['saga', 'lbfgs', 'newton-cg', 'liblinear'],\n",
    "    'max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "lr_random = RandomizedSearchCV(estimator=lr, param_distributions=params, n_iter=10, cv=5, random_state=42, scoring='recall')\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_random.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = lr_random.best_params_\n",
    "print(best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters on the training data\n",
    "lr = LogisticRegression(random_state=42, **best_params)\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the metrics\n",
    "tuned_lr_acc = accuracy_score(y_test, y_pred)\n",
    "tuned_lr_prec = precision_score(y_test, y_pred)\n",
    "tuned_lr_rec = recall_score(y_test, y_pred)\n",
    "tuned_lr_f1 = f1_score(y_test, y_pred)\n",
    "tuned_lr_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tuned_lr_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", tuned_lr_acc)\n",
    "print(\"Precision:\", tuned_lr_prec)\n",
    "print(\"Recall:\", tuned_lr_rec)\n",
    "print(\"F1 score:\", tuned_lr_f1)\n",
    "print(\"ROC-AUC score:\", tuned_lr_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of scores for each model\n",
    "lr_tuned_scores_dict = {\n",
    "    'Model': ['LR: Smote', 'LR: Smote - Tuned'],\n",
    "    'Accuracy': [lr_acc, tuned_lr_acc],\n",
    "    'Precision': [lr_prec, tuned_lr_prec],\n",
    "    'Recall': [lr_rec, tuned_lr_rec],\n",
    "    'F1 Score': [lr_f1, tuned_lr_f1],\n",
    "    'ROC-AUC Score': [lr_roc_auc, tuned_lr_roc_auc]\n",
    "}\n",
    "\n",
    "# create the DataFrame\n",
    "lr_tuned_scores_df = pd.DataFrame(lr_tuned_scores_dict)\n",
    "lr_tuned_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dda475",
   "metadata": {},
   "source": [
    "**Plot the feature importance & the coefficient matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color gradient for the bar chart\n",
    "color_map = plt.cm.get_cmap('BrBG')\n",
    "\n",
    "# Get the absolute coefficients and feature names\n",
    "coefs = np.abs(lr.coef_[0])\n",
    "features = list(X_train.columns)\n",
    "\n",
    "# Sort the coefficients in descending order\n",
    "sorted_idx = coefs.argsort()[::-1]\n",
    "sorted_coefs = coefs[sorted_idx]\n",
    "sorted_features = [features[i] for i in sorted_idx]\n",
    "\n",
    "# Plot the feature importance with a color gradient\n",
    "plt.figure(figsize=(15, 6))\n",
    "bars = plt.bar(sorted_features, sorted_coefs, color=color_map(sorted_coefs / max(sorted_coefs)))\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add title and labels for x-axis & y-axis\n",
    "plt.xlabel('Features', size=12)\n",
    "plt.ylabel('Absolute Coefficients', size=12)\n",
    "plt.title('Few Features Have a Coefficient Above 0.2', size=15)\n",
    "\n",
    "# Add horizontal lines behind the bars.\n",
    "for i in range(0, 9):\n",
    "    plt.axhline(y=i/10, color='lightblue', zorder=-1, linewidth=0.5)\n",
    "    \n",
    "# Add a horizontal line at 0.0\n",
    "plt.axhline(y=0.0, color='lightblue', zorder=-1, linewidth=1.50)\n",
    "\n",
    "# Remove the spines of the plot\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Remove the little ticks on the y-axis\n",
    "plt.tick_params(axis='y', which='both', length=0)\n",
    "\n",
    "# Add space between the bars\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "# Save the figure as a PNG image with a specified resolution and tight bounding box\n",
    "plt.savefig('Images/Feature Importance', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94134e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color gradient for the bar chart\n",
    "color_map = plt.cm.get_cmap('BrBG')\n",
    "\n",
    "# Labels for Xticks\n",
    "x_labels = ['Total of Credit Limits \\nfor Installment Accounts', \\\n",
    "                                         'Total Credit Balance \\n(Except Mortgages)',\\\n",
    "                                         'Total Accounts Opened \\nin the Past 24 Months',\\\n",
    "                                         'Sum of Credit Limits \\nfrom All Credit Lines',\\\n",
    "                                         'Reason for Loan: \\nDebt Consolodation']\n",
    "\n",
    "# Get the absolute coefficients and feature names\n",
    "coefs = np.abs(lr.coef_[0])\n",
    "features = list(X_train.columns)\n",
    "\n",
    "# Sort the coefficients in descending order\n",
    "sorted_idx = coefs.argsort()[::-1]\n",
    "sorted_coefs = coefs[sorted_idx][:5]\n",
    "sorted_features = [features[i] for i in sorted_idx][:5]\n",
    "\n",
    "# Plot the feature importance with a color gradient\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(sorted_features, sorted_coefs, color=color_map(sorted_coefs / max(sorted_coefs)))\n",
    "\n",
    "# Add title and labels for x-axis, y-axis & x-ticks\n",
    "plt.title('Winners: Absolute Coefficients', size=15)\n",
    "plt.xlabel('Features', size=12)\n",
    "plt.ylabel('Absolute Coefficients', size=12)\n",
    "plt.xticks(range(len(sorted_features)), labels=x_labels)\n",
    "\n",
    "# Add horizontal lines behind the bars.\n",
    "for i in range(1, 7):\n",
    "    plt.axhline(y=i/10, color='lightblue', zorder=-1, linewidth=0.5)\n",
    "    \n",
    "# Add a horizontal line at 0.0\n",
    "plt.axhline(y=0.0, color='lightblue', zorder=-1, linewidth=1.50)\n",
    "\n",
    "# Remove the spines of the plot\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Remove the little ticks on the y-axis\n",
    "plt.tick_params(axis='y', which='both', length=0)\n",
    "\n",
    "# Add space between the bars\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "# Save the figure as a PNG image with a specified resolution and tight bounding box\n",
    "plt.savefig('Images/Top Coefficients', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c19088",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features['Absolute Coefficients'] = sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf063c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a07743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the coefficients in descending order\n",
    "sorted_idx = coefs.argsort()[::-1]\n",
    "sorted_features = [features[i] for i in sorted_idx]\n",
    "sorted_coefs = coefs[sorted_idx]\n",
    "\n",
    "# Create a DataFrame with the sorted features and coefficients\n",
    "df_feature_importance = pd.DataFrame({'Feature': sorted_features, 'Coefficient': sorted_coefs})\n",
    "\n",
    "# Print the DataFrame with the feature importance\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdd346",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "# import numpy as np\n",
    "\n",
    "# Define the base colormap\n",
    "cmap_base = plt.cm.get_cmap('BrBG')\n",
    "\n",
    "# Shift the colormap by 30%\n",
    "shifted_cmap = ListedColormap(cmap_base(np.linspace(0.35, 1, 256)))\n",
    "\n",
    "# Plot the confusion matrix using the shifted colormap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=shifted_cmap, alpha=0.85)\n",
    "\n",
    "# Add title and labels for x-axis & y-axis\n",
    "plt.title('Confusion Matrix', size=15)\n",
    "plt.xlabel('Predicted Labels', size=12)\n",
    "plt.ylabel('True Labels', size=12)\n",
    "\n",
    "# Remove the little ticks on the y-axis\n",
    "plt.tick_params(which='both', length=0)\n",
    "\n",
    "# Save the figure as a PNG image with a specified resolution and tight bounding box\n",
    "plt.savefig('Images/Confusion Matrix', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48557c8c",
   "metadata": {},
   "source": [
    "**I decided to see if I can get slightly better results by engineering new features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76900dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271fa3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe that includes new engineered features. \n",
    "df_eng = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature: Proportion of revolving credit in use.\n",
    "# \"credit utilization rate\"  = \"totalRevBal\" / \"totalRevLim\"\n",
    "df_eng[\"credit_utilization_rate\"] = np.round(np.where(df.totalRevLim == 0, 0, df.totalRevBal / df.totalRevLim), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature: Number of credit lines that are revolving credit.\n",
    "df_eng['revolving_credit_lines'] = df.totalAcc * df.revolRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Feature: Loan amount to income ratio.\n",
    "df_eng['amount_to_income_ratio'] = round(df.amount / df.income, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm new features are not redundant to existing. \n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(df_eng.corr().round(2), annot=True, cmap='PuOr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaae3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and then get dummies\n",
    "X2 = df_eng.drop('status', axis=1)\n",
    "y2 = df_eng['status']\n",
    "\n",
    "# Encode y\n",
    "y2 = y.replace({'Current': 0, 'Default': 1})\n",
    "\n",
    "# Create dummies and check new feautures\n",
    "X2 = pd.get_dummies(X2, drop_first=True)\n",
    "\n",
    "# Change uint8 data tpes to int\n",
    "uint8_cols = X2.select_dtypes(include='uint8').columns\n",
    "X2[uint8_cols] = X2[uint8_cols].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de879dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "\n",
    "# initialize a StandardScaler object and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X2_train)\n",
    "\n",
    "# apply the scaling transformation to both the training and test data\n",
    "X2_train_scaled = scaler.transform(X2_train)\n",
    "X2_test_scaled = scaler.transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create oversampled training set using synthetic data\n",
    "smote = SMOTE(random_state=42)\n",
    "X2_train_smote, y2_train_smote = smote.fit_resample(X2_train_scaled, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression object\n",
    "lr = LogisticRegression(class_weight='balanced', random_state=42, **best_params)\n",
    "\n",
    "# Train the model on the training data (scaled data)\n",
    "lr.fit(X2_train_smote, y2_train_smote)\n",
    "\n",
    "# Make predictions on the test data (scaled data)\n",
    "y_pred = lr.predict(X2_test_scaled)\n",
    "\n",
    "# Calculate the metrics\n",
    "eng_lr_acc = accuracy_score(y_test, y_pred)\n",
    "eng_lr_prec = precision_score(y_test, y_pred)\n",
    "eng_lr_rec = recall_score(y_test, y_pred)\n",
    "eng_lr_f1 = f1_score(y_test, y_pred)\n",
    "eng_lr_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", eng_lr_acc)\n",
    "print(\"Precision:\", eng_lr_prec)\n",
    "print(\"Recall:\", eng_lr_rec)\n",
    "print(\"F1 score:\", eng_lr_f1)\n",
    "print(\"ROC-AUC score:\", eng_lr_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of scores for each model\n",
    "lr_eng_scores_dict = {\n",
    "    'Model': ['LR: Smote', 'LR: Smote - Tuned', 'LR: Smote - Tuned + Eng'],\n",
    "    'Accuracy': [lr_acc, tuned_lr_acc, eng_lr_acc],\n",
    "    'Precision': [lr_prec, tuned_lr_prec, eng_lr_prec],\n",
    "    'Recall': [lr_rec, tuned_lr_rec, eng_lr_rec],\n",
    "    'F1 Score': [lr_f1, tuned_lr_f1, eng_lr_f1],\n",
    "    'ROC-AUC Score': [lr_roc_auc, tuned_lr_roc_auc, eng_lr_roc_auc]\n",
    "}\n",
    "\n",
    "# create the DataFrame\n",
    "lr_eng_scores_df = pd.DataFrame(lr_eng_scores_dict)\n",
    "lr_eng_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ed396",
   "metadata": {},
   "source": [
    "**Looks like the new features have yielded worse results so we'll stop looking at that modified database.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
